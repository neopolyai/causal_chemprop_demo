{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astartes import train_test_split\n",
    "from lightning import pytorch as pl\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from causal_chemprop_logS import SobolevMulticomponentMPNN, CustomMSEMetric\n",
    "from chemprop import data as chemprop_data_utils\n",
    "from chemprop import featurizers, nn\n",
    "from chemprop.nn.transforms import ScaleTransform, UnscaleTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 8675309\n",
    "TRAINING_FPATH = Path(\"../data/aqueous.csv\")        # TODO: Update path\n",
    "\n",
    "# setup logging and output directories\n",
    "_output_dir = Path(f\"output/chemprop_{int(datetime.datetime.now(datetime.UTC).timestamp())}\")\n",
    "os.makedirs(_output_dir, exist_ok=True)\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.INFO)\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "downsample_percent = 1.0\n",
    "mpnn_hidden_size = 800\n",
    "fnn_hidden_size = 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _f(r):  # approximates gradients of logS wrt temperature using finite differences\n",
    "    if len(r[\"scaled_logS\"]) == 1:\n",
    "        return [np.nan]\n",
    "    sorted_idxs = np.argsort(r[\"scaled_temperature\"])\n",
    "    unsort_idxs = np.argsort(sorted_idxs)\n",
    "    # mask out enormous (non-physical) values, negative values, and nan/inf\n",
    "    grads = [\n",
    "        i if (np.isfinite(i) and np.abs(i) < 1.0 and i > 0.0) else np.nan\n",
    "        for i in np.gradient(\n",
    "            [r[\"scaled_logS\"][i] for i in sorted_idxs],\n",
    "            [r[\"scaled_temperature\"][i] for i in sorted_idxs],\n",
    "        )\n",
    "    ]\n",
    "    return [grads[i] for i in unsort_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data\n",
    "df = pd.read_csv(TRAINING_FPATH)\n",
    "if downsample_percent:\n",
    "    print(f\"Down-sampling training data to {downsample_percent:.2%} size!\")\n",
    "    downsample_df = df.copy()\n",
    "    downsample_df[\"original_index\"] = np.arange(len(df))\n",
    "    downsample_df = downsample_df.groupby([\"solute_smiles\", \"solvent_smiles\", \"source\"]).aggregate(list)\n",
    "    downsample_df = downsample_df.sample(frac=downsample_percent, replace=False, random_state=RANDOM_SEED)\n",
    "    chosen_indexes = downsample_df.explode(\"original_index\")[\"original_index\"].to_numpy().flatten().astype(int)\n",
    "    print(f\"Actual downsample percentage is {len(chosen_indexes)/len(df):.2%}, count: {len(chosen_indexes)}!\")\n",
    "    df = df.iloc[chosen_indexes]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# split the data s.t. model only sees a subset of the studies used to aggregate the training data\n",
    "studies_train, studies_val = train_test_split(pd.unique(df[\"source\"]), random_state=RANDOM_SEED, train_size=0.9, test_size=0.1)       \n",
    "train_indexes = df.index[df[\"source\"].isin(studies_train)].tolist()\n",
    "val_indexes = df.index[df[\"source\"].isin(studies_val)].tolist()\n",
    "_total = len(df)\n",
    "print(f\"train: {len(train_indexes)} ({len(train_indexes)/_total:.0%}) validation:\" f\"{len(val_indexes)} ({len(val_indexes)/_total:.0%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual re-scaling\n",
    "target_scaler = StandardScaler().fit(df[[\"logS\"]].iloc[train_indexes])\n",
    "scaled_logs = target_scaler.transform(df[[\"logS\"]]).ravel()\n",
    "temperature_scaler = StandardScaler().fit(df[[\"temperature\"]].iloc[train_indexes])\n",
    "scaled_temperature = temperature_scaler.transform(df[[\"temperature\"]]).ravel()\n",
    "# calculate known temperature gradients\n",
    "tgrads = pd.concat(\n",
    "    (\n",
    "        df,\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"source_index\": np.arange(len(df[\"temperature\"])),\n",
    "                \"scaled_temperature\": scaled_temperature,\n",
    "                \"scaled_logS\": scaled_logs,\n",
    "            }\n",
    "        ),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "# group the data by experiment\n",
    "tgrads = tgrads.groupby([\"source\", \"solvent_smiles\", \"solute_smiles\"])[[\"scaled_logS\", \"scaled_temperature\", \"source_index\"]].aggregate(list)\n",
    "tgrads[\"logSgradT\"] = tgrads.apply(_f, axis=1)\n",
    "tgrads = tgrads.explode([\"logSgradT\", \"source_index\"]).sort_values(by=\"source_index\")\n",
    "tgrads = tgrads[\"logSgradT\"].to_numpy(dtype=np.float32)\n",
    "_mask = np.isnan(tgrads)\n",
    "print(f\"Masking {np.count_nonzero(_mask)} of {len(_mask)} gradients!\")\n",
    "print(f\"{np.count_nonzero(tgrads > 0)} of {len(tgrads)} were positive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [\n",
    "    [\n",
    "        chemprop_data_utils.MoleculeDatapoint.from_smi(smi, [log_s, log_s_grad_T], x_d=np.array([temperature]))\n",
    "        for smi, log_s, log_s_grad_T, temperature in zip(df[\"solute_smiles\"], scaled_logs, tgrads, df[\"temperature\"])\n",
    "    ],\n",
    "    list(map(chemprop_data_utils.MoleculeDatapoint.from_smi, df[\"solvent_smiles\"])),\n",
    "]\n",
    "# create datasets\n",
    "train_data, val_data, _ = chemprop_data_utils.split_data_by_indices(all_data, train_indices=train_indexes, val_indices=val_indexes)\n",
    "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "train_datasets = [chemprop_data_utils.MoleculeDataset(train_data[i], featurizer) for i in range(len(all_data))]\n",
    "val_datasets = [chemprop_data_utils.MoleculeDataset(val_data[i], featurizer) for i in range(len(all_data))]\n",
    "causal_datasets = [chemprop_data_utils.MoleculeDataset(train_data[i], featurizer) for i in range(len(all_data))]       \n",
    "# Create mcdatasets\n",
    "train_mcdset = chemprop_data_utils.MulticomponentDataset(train_datasets)\n",
    "train_mcdset.normalize_inputs(\"X_d\", [temperature_scaler, None])\n",
    "train_mcdset.cache = True\n",
    "val_mcdset = chemprop_data_utils.MulticomponentDataset(val_datasets)\n",
    "val_mcdset.cache = True\n",
    "causal_mcdset = chemprop_data_utils.MulticomponentDataset(causal_datasets)\n",
    "causal_mcdset.cache = True\n",
    "# Create loaders\n",
    "train_loader = chemprop_data_utils.build_dataloader(train_mcdset, batch_size=256, num_workers=1, persistent_workers=True, shuffle=True)\n",
    "val_loader = chemprop_data_utils.build_dataloader(val_mcdset, batch_size=32, num_workers=1, persistent_workers=True, shuffle=False)\n",
    "causal_loader = chemprop_data_utils.build_dataloader(causal_mcdset, batch_size=32, num_workers=1, persistent_workers=True, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Chemprop\n",
    "mcmp = nn.MulticomponentMessagePassing(\n",
    "    blocks=[nn.BondMessagePassing(depth=3, d_h=mpnn_hidden_size, dropout=0.50) for _ in range(len(all_data))],\n",
    "    n_components=len(all_data),\n",
    ")\n",
    "agg = nn.MeanAggregation()\n",
    "output_transform = UnscaleTransform.from_standard_scaler(target_scaler)\n",
    "ffn = nn.RegressionFFN(\n",
    "    input_dim=mcmp.output_dim + 1,  # temperature\n",
    "    hidden_dim=fnn_hidden_size,\n",
    "    dropout=0.50,\n",
    "    n_layers=4,\n",
    "    criterion=CustomMSEMetric(),\n",
    "    output_transform=output_transform,\n",
    ")\n",
    "X_d_transform = ScaleTransform.from_standard_scaler(temperature_scaler)\n",
    "metric_list = [CustomMSEMetric()]\n",
    "mcmpnn = SobolevMulticomponentMPNN(\n",
    "    fnn_hidden_size + 1,  # +1 for solubility\n",
    "    _output_dir,\n",
    "    mcmp,\n",
    "    agg,\n",
    "    ffn,\n",
    "    batch_norm=True,\n",
    "    metrics=metric_list,\n",
    "    X_d_transform=X_d_transform,\n",
    "    init_lr=0.00001,     \n",
    "    max_lr=0.0001,\n",
    "    final_lr=0.00001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure trainer\n",
    "tensorboard_logger = TensorBoardLogger(\n",
    "    _output_dir,\n",
    "    name=\"tensorboard_logs\",\n",
    "    default_hp_metric=False,\n",
    ")\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val/logs_loss\",  \n",
    "        mode=\"min\",\n",
    "        verbose=False,\n",
    "        patience=10,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        monitor=\"val/logs_loss\",\n",
    "        dirpath=os.path.join(_output_dir, \"checkpoints\"),\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    ),\n",
    "]\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    max_epochs=200,           \n",
    "    logger=tensorboard_logger,\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=True,\n",
    "    check_val_every_n_epoch=1,\n",
    "    callbacks=callbacks,\n",
    "    num_sanity_val_steps=0,\n",
    "    inference_mode=False,       # to enable sobolev\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit MCMPNN and causal model\n",
    "trainer.fit(mcmpnn, train_loader, val_loader)\n",
    "ckpt_path = trainer.checkpoint_callback.best_model_path\n",
    "mcmpnn = mcmpnn.__class__.load_from_checkpoint(ckpt_path)\n",
    "mcmpnn.fit_causal(causal_loader, causal_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.manifold import TSNE\n",
    "from lightning.pytorch import Trainer\n",
    "\n",
    "from causal_chemprop_logS import SobolevMulticomponentMPNN\n",
    "import evomol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parity_plot(truth, prediction, title):\n",
    "    r, _ = pearsonr(truth, prediction)\n",
    "    mse = mean_squared_error(truth, prediction)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(truth, prediction)\n",
    "    wn_07 = np.count_nonzero(np.abs(truth - prediction) <= 0.7) / len(prediction)\n",
    "    wn_1 = np.count_nonzero(np.abs(truth - prediction) <= 1.0) / len(prediction)\n",
    "    \n",
    "    stat_str = (f\" - RMSE: {rmse:.4f}\\n - W/n 0.7: {wn_07:.4f}\\n\")\n",
    "    # stat_str = f\" - Pearson's r: {r:.4f}\\n - RMSE: {rmse:.4f}\"\n",
    "    plt.clf()\n",
    "    plt.scatter(\n",
    "        truth, prediction,\n",
    "        alpha=0.5,              \n",
    "        s=25,                   \n",
    "        edgecolors='black',     \n",
    "        facecolors='blue',      \n",
    "        marker='o'\n",
    "    )\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.xlabel(\"Actual logS values\", fontsize=14)\n",
    "    plt.ylabel(\"Predicted logS values\", fontsize=14)\n",
    "    min_val = min(np.min(truth), np.min(prediction)) - 0.5\n",
    "    max_val = max(np.max(truth), np.max(prediction)) + 0.5\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color=\"black\", linestyle=\"-\")\n",
    "    plt.plot([min_val, max_val], [min_val + 1, max_val + 1], color=\"red\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.plot([min_val, max_val], [min_val - 1, max_val - 1], color=\"red\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.xlim(min_val, max_val)\n",
    "    plt.ylim(min_val, max_val)\n",
    "    plt.text(\n",
    "        min_val, max_val - 0.1, stat_str,\n",
    "        ha=\"left\", va=\"top\",\n",
    "        fontsize=14,\n",
    "        bbox=dict(facecolor='white', alpha=0.01, edgecolor='none')\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MCMPNN and causal model\n",
    "ckpt_path = Path(\"output/chemprop_aqueous/checkpoints/epoch=16-step=170.ckpt\")      # TODO: Update path\n",
    "causal_pkl = Path(\"output/chemprop_aqueous/causal_model_histogram.pkl\")             # TODO: Update path\n",
    "model = SobolevMulticomponentMPNN.load_from_checkpoint(\n",
    "    ckpt_path, \n",
    "    causal_pkl=causal_pkl\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test _causal_inference\n",
    "for holdout_fpath in (\n",
    "    Path(\"../data/aqueous.csv\"),    # TODO: Update path\n",
    "):\n",
    "    df = pd.read_csv(holdout_fpath)\n",
    "    df = df.loc[val_indexes]       \n",
    "    test_datapoints = [\n",
    "        [\n",
    "            chemprop_data_utils.MoleculeDatapoint.from_smi(smi, None, x_d=np.array([temperature]))\n",
    "            for smi, temperature in zip(df[\"solute_smiles\"], df[\"temperature\"])\n",
    "        ],\n",
    "        list(map(chemprop_data_utils.MoleculeDatapoint.from_smi, df[\"solvent_smiles\"])),\n",
    "    ]\n",
    "    featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "    test_datasets = [chemprop_data_utils.MoleculeDataset(test_datapoints[i], featurizer) for i in range(len(test_datapoints))]\n",
    "    test_mcdset = chemprop_data_utils.MulticomponentDataset(test_datasets)\n",
    "    test_loader = chemprop_data_utils.build_dataloader(test_mcdset, shuffle=False)\n",
    "    trainer = Trainer(logger=False)\n",
    "    predictions = np.concatenate(trainer.predict(model, test_loader), axis=0)\n",
    "    parity_plot(df[\"logS\"], predictions, holdout_fpath.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test _causal_counterfactual\n",
    "X, Y = [], []\n",
    "for holdout_fpath in (\n",
    "    Path(\"../data/mif.csv\"),        # TODO: Update path\n",
    "):\n",
    "    mif_df = pd.read_csv(holdout_fpath)\n",
    "    for i in range(1, len(mif_df)):\n",
    "        df = mif_df.iloc[[0, i]]  \n",
    "        test_datapoints = [\n",
    "            [\n",
    "                chemprop_data_utils.MoleculeDatapoint.from_smi(smi, None, x_d=np.array([temperature]))\n",
    "                for smi, temperature in zip(df[\"solute_smiles\"], df[\"temperature\"])\n",
    "            ],\n",
    "            list(map(chemprop_data_utils.MoleculeDatapoint.from_smi, df[\"solvent_smiles\"])),\n",
    "        ]\n",
    "        featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "        test_datasets = [chemprop_data_utils.MoleculeDataset(test_datapoints[i], featurizer) for i in range(len(test_datapoints))]\n",
    "        test_mcdset = chemprop_data_utils.MulticomponentDataset(test_datasets)\n",
    "        test_loader = chemprop_data_utils.build_dataloader(test_mcdset, shuffle=False)\n",
    "        trainer = Trainer(logger=False)\n",
    "        predictions = np.concatenate(trainer.predict(model, test_loader), axis=0)\n",
    "        X.append(df[\"logS\"].iloc[-1])\n",
    "        Y.append(predictions[-1])\n",
    "        print(df[\"solute_smiles\"].iloc[-1], df[\"logS\"].iloc[-1], predictions[-1])\n",
    "    parity_plot(np.array(X).reshape(-1,1), np.array(Y).reshape(-1,1), \"NNRTI\")\n",
    "\n",
    "\n",
    "seed_sol = target_scaler.transform([[-2.11742423766986]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test _causal_generate\n",
    "evomol.run_model({\n",
    "    # Adjust weight ratio between solubility and plogp \n",
    "    \"obj_function\": {\n",
    "        \"type\": \"linear_combination\",\n",
    "        \"functions\": [\"qed\", \"plogp\", \"norm_sascore\"],\n",
    "        \"coef\": [1.0, 0.0, 0.0]\n",
    "    },            \n",
    "    \"optimization_parameters\": {        \n",
    "        \"max_steps\": 50,\n",
    "        \"model\": model,   \n",
    "    },\n",
    "    # Constraint action space to preserve core\n",
    "    \"action_space_parameters\": {\n",
    "        \"atoms\": \"C,N,O,F\",\n",
    "        \"change_bond_prevent_breaking_creating_bonds\": True,\n",
    "        \"remove_group_only_remove_smallest_group\": False\n",
    "    },\n",
    "    # Adjust seed molecules to perturb\n",
    "    \"io_parameters\": {\n",
    "        \"model_path\": \"output/neopoly\",\n",
    "        \"smiles_list_init\": [\n",
    "            \"C1=CC=C2C(=C1)C=CC(=N2)C3=CN(N=N3)C4=CC=C(C=C4)O\",\n",
    "            \"C1=CC(=CC=C1N2C=C(N=N2)C3=NC4=C(C=C3)C=C(C=C4)OCCO)O\",\n",
    "            \"C1=CC(=CC=C1N2C=C(N=N2)C3=NC4=C(C=C3)C=C(C=C4)OCCN)O\",\n",
    "            \"COCCOC1=CC2=C(C=C1)N=C(C=C2)C3=CN(N=N3)C4=CC=C(C=C4)O\",\n",
    "            \"C1=CC(=CC=C1N2C=C(N=N2)C3=NC4=C(C=C3)C=C(C=C4)OCCOCCN)O\",\n",
    "            \"C1COCCN1CCOCCOC2=CC3=C(C=C2)N=C(C=C3)C4=CN(N=N4)C5=CC=C(C=C5)O\",\n",
    "            \"C1=CC(=CC=C1N2C=C(N=N2)C3=NC4=C(C=C3)C=C(C=C4)OCC(=O)O)O\",\n",
    "            \"C1=CC(=C(C=C1N2C=C(N=N2)C3=NC4=C(C=C3)C=C(C=C4)OCCOCCN)F)O\",\n",
    "            \"C1COCCN1CCOCCOC2=CC3=C(C=C2)N=C(C=C3)C4=CN(N=N4)C5=CC(=C(C=C5)O)F\",\n",
    "            \"C1=CC(=C(C=C1N2C=C(N=N2)C3=NC4=C(C=C3)C=C(C=C4)OCC(=O)O)F)O\",\n",
    "            \"C1=CC(=C(C=C1N2C=C(N=N2)C3=NC4=C(C=C3)C=C(C=C4)OCCCC(=O)O)F)O\",\n",
    "            \"C1=CC(=C(C=C1N2C=C(N=N2)C3=NC4=C(C=C3)C=C(C=C4)OCCOCC(=O)O)F)O\",\n",
    "            \"OC(=O)COc1cc2nccc(c2cc1)c3cn(nn3)c4ccccc4\",\n",
    "            \"COCCOc1ccc(cc1)Oc2ccnc3cc(-c4cnnn4c5ccc(F)cc5O)ccc23\",\n",
    "            \"O=C(O)c1ccc(cc1)Oc2ccnc3cc(-c4cnnn4c5ccc(F)cc5O)ccc23\"\n",
    "        ]\n",
    "    },   \n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
